{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.document_stores import ElasticsearchDocumentStore\n",
    "import os\n",
    "from haystack import Pipeline\n",
    "from haystack.nodes import PreProcessor\n",
    "from src import download_data, config, preprocess_data_raw, preprocess_data_raw_2\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(format=\"%(levelname)s - %(name)s -  %(message)s\", level=logging.WARNING)\n",
    "logging.getLogger(\"haystack\").setLevel(logging.INFO)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data for AWS bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = config.BUCKET_NAME\n",
    "s3_folder = config.S3_FOLDER_URL\n",
    "local_dir = config.DATA_RAW_PATH\n",
    "\n",
    "# load data form AWS bucket and save in local_dir\n",
    "download_data.download_s3_folder(bucket_name, s3_folder, \n",
    "                                 local_dir=local_dir, \n",
    "                                 sample_size=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save data set as jason in .data/processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - haystack.utils.preprocessing -  Converting d:\\Alexander\\Cursos Programación\\AnyoneAI ML\\Final Project\\financial-advisor-chatbot\\data\\raw\\ameri-holdings-inc\\NASDAQ_AMRH_2017.pdf\n",
      "INFO - haystack.utils.preprocessing -  Converting d:\\Alexander\\Cursos Programación\\AnyoneAI ML\\Final Project\\financial-advisor-chatbot\\data\\raw\\ameri-holdings-inc\\NASDAQ_AMRH_2018.pdf\n",
      "INFO - haystack.utils.preprocessing -  Converting d:\\Alexander\\Cursos Programación\\AnyoneAI ML\\Final Project\\financial-advisor-chatbot\\data\\raw\\copart\\NASDAQ_CPRT_2018.pdf\n",
      "INFO - haystack.utils.preprocessing -  Converting d:\\Alexander\\Cursos Programación\\AnyoneAI ML\\Final Project\\financial-advisor-chatbot\\data\\raw\\core-mark-holding-company-inc\\NASDAQ_CORE_2018.pdf\n",
      "INFO - haystack.utils.preprocessing -  Converting d:\\Alexander\\Cursos Programación\\AnyoneAI ML\\Final Project\\financial-advisor-chatbot\\data\\raw\\gencor-industries-inc\\NASDAQ_GENC_2021.pdf\n",
      "INFO - haystack.utils.preprocessing -  Converting d:\\Alexander\\Cursos Programación\\AnyoneAI ML\\Final Project\\financial-advisor-chatbot\\data\\raw\\nicolet-bankshares-inc\\NASDAQ_NCBS_2020.pdf\n",
      "INFO - haystack.utils.preprocessing -  Converting d:\\Alexander\\Cursos Programación\\AnyoneAI ML\\Final Project\\financial-advisor-chatbot\\data\\raw\\orthofix-international-nv\\NASDAQ_OFIX_2019.pdf\n",
      "INFO - haystack.utils.preprocessing -  Converting d:\\Alexander\\Cursos Programación\\AnyoneAI ML\\Final Project\\financial-advisor-chatbot\\data\\raw\\scholastic-corporation\\NASDAQ_SCHL_2019.pdf\n",
      "INFO - haystack.utils.preprocessing -  Converting d:\\Alexander\\Cursos Programación\\AnyoneAI ML\\Final Project\\financial-advisor-chatbot\\data\\raw\\seagate-technology-plc\\NASDAQ_STX_2018.pdf\n",
      "INFO - haystack.utils.preprocessing -  Converting d:\\Alexander\\Cursos Programación\\AnyoneAI ML\\Final Project\\financial-advisor-chatbot\\data\\raw\\steel-dynamics-inc\\NASDAQ_STLD_2018.pdf\n"
     ]
    }
   ],
   "source": [
    "preprocess_data_raw_2.convert_pdf_jason(config.DATA_RAW_PATH, config.DATA_PROCESSED_PATH)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generated the Elastisearch document store\n",
    "Before we need to run the elastisearch container using just once:\n",
    "\n",
    "**docker pull docker.elastic.co/elasticsearch/elasticsearch:7.9.2**\n",
    "\n",
    "And then run image:\n",
    "\n",
    "**docker run -d -p 9200:9200 -e \"discovery.type=single-node\" elasticsearch:7.9.2**\n",
    "\n",
    "You can do this manually, or using our [launch_es()](https://docs.haystack.deepset.ai/reference/utils-api) utility function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the host where Elasticsearch is running, default to localhost\n",
    "host = os.environ.get(\"ELASTICSEARCH_HOST\", \"localhost\")\n",
    "\n",
    "document_store = ElasticsearchDocumentStore(\n",
    "    host=host,\n",
    "    username=\"\",\n",
    "    password=\"\",\n",
    "    index=\"document\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset from json_dataset.json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file_path = os.path.join(config.DATA_PROCESSED_PATH, \"json_dataset.json\")\n",
    "# Load the documents from the JSON file\n",
    "with open(json_file_path, \"r\") as f:\n",
    "    document_list = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_list[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing and loading into the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "959dd9472ffd4f228409331fb88d7b36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preprocessing:   0%|          | 0/10 [00:00<?, ?docs/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - haystack.nodes.preprocessor.preprocessor -  Document ce803e014fb54e0713db190d4857f33 is 166040 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time.\n",
      "WARNING - haystack.nodes.preprocessor.preprocessor -  Document f0a84d290964abe8583b799bfa63f0fa is 162796 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time.\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of PreProcessor\n",
    "# Each document is divided into paragraphs of approximately 500 tokens.\n",
    "\n",
    "preprocessor = PreProcessor(\n",
    "    clean_empty_lines=True,\n",
    "    clean_whitespace=True,\n",
    "    clean_header_footer=True,\n",
    "    split_by=\"word\",\n",
    "    split_length=200,\n",
    "    split_overlap=20,\n",
    "    split_respect_sentence_boundary=False,\n",
    ")\n",
    "\n",
    "doc_processed = preprocessor.process(document_list)\n",
    "\n",
    "# clear the document store before loading documents\n",
    "document_store.delete_documents()\n",
    "\n",
    "# Upload documents to the document store\n",
    "document_store.write_documents(doc_processed)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate retriver and PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.nodes import BM25Retriever, SentenceTransformersRanker, PromptTemplate, AnswerParser, PromptNode\n",
    "# Create an instance of BM25Retriver to select top_k better documents into document_store\n",
    "retriever = BM25Retriever(document_store=document_store, top_k = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - haystack.modeling.utils -  Using devices: CPU - Number of GPUs: 0\n",
      "d:\\Alexander\\Cursos Programación\\AnyoneAI ML\\Final Project\\financial-advisor-chatbot\\.venv\\Lib\\site-packages\\torch\\_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "# ranked to have a better sorted list of relevant documents using a Cross-Encoder model .\n",
    "ranker = SentenceTransformersRanker(model_name_or_path=\"cross-encoder/ms-marco-MiniLM-L-12-v2\", top_k = 3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using GPT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline Retriver-Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_template = PromptTemplate(\n",
    "    name=\"question-answering-with-references\",\n",
    "    prompt_text=\"Create a concise and informative answer (no more than 50 words) for a given question \"\n",
    "    \"based solely on the given documents. You must only use information from the given documents. \"\n",
    "    \"Use a financial assistant tone and be very specific with amounts. Do not repeat text. Cite the documents using Document[number] notation. \"\n",
    "    \"If multiple documents contain the answer, cite those documents like ‘as stated in Document[number], Document[number], etc.’. \"\n",
    "    \"If the documents do not contain the answer to the question, say that ‘answering is not possible given the available information.’\\n\"\n",
    "    \"{join(documents, delimiter=new_line, pattern=new_line+'Document[$idx]: $content', str_replace={new_line: ' ', '[': '(', ']': ')'})} \\n Question: {query}; Answer: \",\n",
    "    output_parser=AnswerParser(reference_pattern=r\"Document\\[(\\d+)\\]\"),)\n",
    "\n",
    "document_prompt_node = PromptNode(\n",
    "        \"gpt-3.5-turbo\",\n",
    "        api_key=config.API_KEY,\n",
    "        max_length=256,\n",
    "        default_prompt_template= tool_template\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.nodes import Shaper, DocumentMerger\n",
    "# Doing our own pipeline\n",
    "\n",
    "# shaper is used to join all documents returned by retriver\n",
    "# shaper = Shaper(func=\"join_documents\", inputs={\"documents\": \"documents\"},outputs=[\"documents\"])\n",
    "\n",
    "#document_merger = DocumentMerger(separator=\" \\n Conten: \\n\")\n",
    "\n",
    "# creating a pipeline\n",
    "pipeline = Pipeline()\n",
    "\n",
    "# Add retriver node (BM25retriver)\n",
    "pipeline.add_node(component=retriever, name=\"Retriever\", inputs=[\"Query\"])\n",
    "\n",
    "# Ranker to get a sorted list of relevant documents\n",
    "pipeline.add_node(component=ranker, name=\"Ranker\", inputs=[\"Retriever\"])\n",
    "\n",
    "# Add shaper node to join all documents\n",
    "#pipeline.add_node(component=document_merger, name=\"DocumentMerger\", inputs=[\"Ranker\"])\n",
    "#pipeline.add_node(component=shaper, name=\"Shaper\", inputs=[\"Ranker\"])\n",
    "\n",
    "# generative\n",
    "pipeline.add_node(component=document_prompt_node, name=\"Generative\", inputs=[\"Ranker\"])\n",
    "\n",
    "\n",
    "# only the \"recall_single_hit\" metric will be considered to evaluate the performance of the Retriever\n",
    "#pipeline.metrics_filter = {\"Retriever\": [\"recall_single_hit\"]}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chek the pipeline\n",
    "query=\"What was Microstrategy's net income in 2019?\" #\"What is the current stock price of seagate-technology-plc?\"\n",
    "result = pipeline.run(query=query, params={\"Retriever\": {\"top_k\": 10}, \"Ranker\":{\"top_k\":3}}) # \n",
    "result['answers']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.agents import Agent, Tool\n",
    "\n",
    "# Let's create the prompt for the Agent's PromptNode\n",
    "# See the full example at https://github.com/deepset-ai/haystack/blob/main/examples/agent_multihop_qa.py\n",
    "\n",
    "zero_shot_react_template=PromptTemplate(\n",
    "            name=\"zero-shot-react\", \n",
    "            prompt_text=\"You are a chatbot working as a financial assistant and you are having a conversation with a user.\"\n",
    "            \"At the end of the conversation, the user tells you something which is usually a new question. \"\n",
    "            \"If you don't know the answer, to achieve your goal of answering complex questions correctly, you have access to the following tool:\\n\\n\"\n",
    "            \"Search_in_documents: useful for when you need to get answers. \\n\\n\"\n",
    "            \"To answer questions, you'll need to go through multiple steps involving step-by-step. \"\n",
    "            \"thinking and selecting when to use the Search_in_documents tool; tool will respond with Observation.\"\n",
    "            \"If after searching you feel that the user should enter any additional data, return an answer requesting it.\"\n",
    "            \"When you are ready for a final answer, respond with the `Final Answer:`\\n\\n\"\n",
    "            \"Use the following format:\\n\\n\"\n",
    "            \"Question: the question to be answered\\n\"\n",
    "            \"Thought: Reason if you have the final answer. If yes, answer the question. If not, make an appropriate query to the Search_in_documents tool.\\n\"\n",
    "            \"Tool: Search_in_documents \\n\"\n",
    "            \"Tool Input: query to search for the missing information needed to answer \\n\"\n",
    "            \"Observation: The tool will respond with the answer \\n\"\n",
    "            \"...\\n\"\n",
    "            \"Final Answer: the final answer to the question, in no more than 50 words\\n\\n\"\n",
    "            \"Thought, Tool, Tool Input, and Observation steps can be repeated multiple times, but sometimes we can find an answer in the first pass\\n\"\n",
    "            \"---\\n\\n\"\n",
    "            \"Question: {query}\\n\"\n",
    "            \"Thought: Let's think step-by-step, I first need to \", \n",
    "        ) # output_parser=AnswerParser()\n",
    "\n",
    "# PromptNode using GPT\n",
    "agent_prompt_node = PromptNode(\n",
    "    \"gpt-3.5-turbo\", api_key=config.API_KEY, max_length=500, stop_words=[\"Observation:\"],\n",
    "    model_kwargs={\"stream\":False, 'temperature':0} # max_length=512, stop_words=[\"Observation:\"],\n",
    ")\n",
    "\n",
    "# Let's configure Search as the Agent's tool\n",
    "# Each tool needs to have a description that helps the Agent decide when to use it\n",
    "search_in_ds = Tool(\n",
    "    name=\"Search_in_documents\",\n",
    "    pipeline_or_node=pipeline,\n",
    "    description=\"useful for when you need to get answers.\",\n",
    "    output_variable='answers',\n",
    ")\n",
    "\n",
    "# Time to initialize the Agent specifying the PromptNode to use and the Tools\n",
    "agent = Agent(\n",
    "    prompt_node=agent_prompt_node,\n",
    "    prompt_template=zero_shot_react_template, \n",
    "    final_answer_pattern= r\"Final Answer\\s*:\\s*(.*)\", \n",
    ")\n",
    "\n",
    "agent.add_tool(tool=search_in_ds)\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procesing the list of dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def use_list_dict(list_dict, new_sentence = None, isquery=True):\n",
    "     \"\"\" \n",
    "     Processing the list of dictionary. If new_sentence is not added then just return the string query.\n",
    "     Otherwise, it appends the new sentence to the list in the role determined by isquery\n",
    "\n",
    "     Parameters\n",
    "    ----------\n",
    "    list_dict: list[dict]\n",
    "        list of dictionary\n",
    "     new_sentence: str\n",
    "        new sentence tha can be an user query or sytem answer\n",
    "     isquery: bool\n",
    "        True -> new_sentence is a query / False -> new_sentence is an answer\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    query_prompt (str) \n",
    "          if new_sentence in None\n",
    "    list_dict: list[dict]\n",
    "          list of dictionary with new sentence append\n",
    "    \"\"\"\n",
    "     \n",
    "     if not new_sentence:\n",
    "         query_prompt = ' \\n '.join([f\"{item['role']}: {item['conten']}\" for item in list_dict])\n",
    "         return query_prompt\n",
    "     if isquery:\n",
    "          list_dict.append({'role':'user', 'conten':new_sentence})\n",
    "     else:\n",
    "          list_dict.append({'role':'assistant', 'conten':new_sentence})\n",
    "\n",
    "     return list_dict\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Who founded Microstrategy?',\n",
       " 'answer': 'Michael J. Saylor, Sanju Bansal, and Thomas Spahr'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_file_path = os.path.join(config.DATA_PROCESSED_PATH, \"qa_dataset.json\")\n",
    "# Load the documents from the JSON file\n",
    "with open(json_file_path, \"r\") as f:\n",
    "    qa_list = json.load(f)\n",
    "\n",
    "qa_list[28]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulating chatBot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Agent zero-shot-react started with {'query': 'user: Who founded Microstrategy?', 'params': None}\n",
      "\u001b[32mcheck\u001b[0m\u001b[32m if\u001b[0m\u001b[32m I\u001b[0m\u001b[32m have\u001b[0m\u001b[32m this\u001b[0m\u001b[32m information\u001b[0m\u001b[32m in\u001b[0m\u001b[32m my\u001b[0m\u001b[32m database\u001b[0m\u001b[32m.\u001b[0m\u001b[32m If\u001b[0m\u001b[32m not\u001b[0m\u001b[32m,\u001b[0m\u001b[32m I\u001b[0m\u001b[32m can\u001b[0m\u001b[32m use\u001b[0m\u001b[32m the\u001b[0m\u001b[32m Search\u001b[0m\u001b[32m_in\u001b[0m\u001b[32m_documents\u001b[0m\u001b[32m tool\u001b[0m\u001b[32m to\u001b[0m\u001b[32m find\u001b[0m\u001b[32m the\u001b[0m\u001b[32m answer\u001b[0m\u001b[32m.\n",
      "\u001b[0m\u001b[32mTool\u001b[0m\u001b[32m:\u001b[0m\u001b[32m Search\u001b[0m\u001b[32m_in\u001b[0m\u001b[32m_documents\u001b[0m\u001b[32m\n",
      "\u001b[0m\u001b[32mTool\u001b[0m\u001b[32m Input\u001b[0m\u001b[32m:\u001b[0m\u001b[32m \"\u001b[0m\u001b[32mMicro\u001b[0m\u001b[32mstrategy\u001b[0m\u001b[32m founder\u001b[0m\u001b[32m\"\n",
      "\u001b[0mObservation: \u001b[33manswering is not possible given the available information.\u001b[0m\n",
      "Thought: \u001b[32mI\u001b[0m\u001b[32m need\u001b[0m\u001b[32m more\u001b[0m\u001b[32m information\u001b[0m\u001b[32m to\u001b[0m\u001b[32m answer\u001b[0m\u001b[32m this\u001b[0m\u001b[32m question\u001b[0m\u001b[32m.\u001b[0m\u001b[32m Can\u001b[0m\u001b[32m you\u001b[0m\u001b[32m provide\u001b[0m\u001b[32m any\u001b[0m\u001b[32m additional\u001b[0m\u001b[32m details\u001b[0m\u001b[32m or\u001b[0m\u001b[32m context\u001b[0m\u001b[32m?\n",
      "\u001b[0m\u001b[32mFinal\u001b[0m\u001b[32m Answer\u001b[0m\u001b[32m:\u001b[0m\u001b[32m N\u001b[0m\u001b[32m/A\u001b[0m\u001b[32m,\u001b[0m\u001b[32m additional\u001b[0m\u001b[32m information\u001b[0m\u001b[32m needed\u001b[0m\u001b[32m.\u001b[0m\n",
      " Who founded Microstrategy?  \n",
      " N/A, additional information needed.\n",
      "\n",
      "Agent zero-shot-react started with {'query': 'user: Who founded Microstrategy? \\n assistant: N/A, additional information needed. \\n user: Who founded Microstrategy companie?', 'params': None}\n",
      "\u001b[32msearch\u001b[0m\u001b[32m for\u001b[0m\u001b[32m the\u001b[0m\u001b[32m answer\u001b[0m\u001b[32m using\u001b[0m\u001b[32m the\u001b[0m\u001b[32m Search\u001b[0m\u001b[32m_in\u001b[0m\u001b[32m_documents\u001b[0m\u001b[32m tool\u001b[0m\u001b[32m.\n",
      "\u001b[0m\u001b[32mTool\u001b[0m\u001b[32m:\u001b[0m\u001b[32m Search\u001b[0m\u001b[32m_in\u001b[0m\u001b[32m_documents\u001b[0m\u001b[32m\n",
      "\u001b[0m\u001b[32mTool\u001b[0m\u001b[32m Input\u001b[0m\u001b[32m:\u001b[0m\u001b[32m \"\u001b[0m\u001b[32mMicro\u001b[0m\u001b[32mstrategy\u001b[0m\u001b[32m founder\u001b[0m\u001b[32m\"\n",
      "\u001b[0mObservation: \u001b[33manswering is not possible given the available information.\u001b[0m\n",
      "Thought: \u001b[32mI\u001b[0m\u001b[32m need\u001b[0m\u001b[32m to\u001b[0m\u001b[32m refine\u001b[0m\u001b[32m my\u001b[0m\u001b[32m search\u001b[0m\u001b[32m query\u001b[0m\u001b[32m to\u001b[0m\u001b[32m get\u001b[0m\u001b[32m a\u001b[0m\u001b[32m more\u001b[0m\u001b[32m specific\u001b[0m\u001b[32m answer\u001b[0m\u001b[32m.\n",
      "\u001b[0m\u001b[32mTool\u001b[0m\u001b[32m:\u001b[0m\u001b[32m Search\u001b[0m\u001b[32m_in\u001b[0m\u001b[32m_documents\u001b[0m\u001b[32m\n",
      "\u001b[0m\u001b[32mTool\u001b[0m\u001b[32m Input\u001b[0m\u001b[32m:\u001b[0m\u001b[32m \"\u001b[0m\u001b[32mWho\u001b[0m\u001b[32m is\u001b[0m\u001b[32m the\u001b[0m\u001b[32m founder\u001b[0m\u001b[32m of\u001b[0m\u001b[32m Micro\u001b[0m\u001b[32mstrategy\u001b[0m\u001b[32m company\u001b[0m\u001b[32m?\"\n",
      "\u001b[0mObservation: \u001b[33mThe documents do not provide information on the founder of MicroStrategy. Answering is not possible given the available information.\u001b[0m\n",
      "Thought: \u001b[32mI\u001b[0m\u001b[32m need\u001b[0m\u001b[32m to\u001b[0m\u001b[32m try\u001b[0m\u001b[32m a\u001b[0m\u001b[32m different\u001b[0m\u001b[32m approach\u001b[0m\u001b[32m to\u001b[0m\u001b[32m find\u001b[0m\u001b[32m the\u001b[0m\u001b[32m answer\u001b[0m\u001b[32m.\n",
      "\u001b[0m\u001b[32mTool\u001b[0m\u001b[32m:\u001b[0m\u001b[32m Search\u001b[0m\u001b[32m_in\u001b[0m\u001b[32m_documents\u001b[0m\u001b[32m\n",
      "\u001b[0m\u001b[32mTool\u001b[0m\u001b[32m Input\u001b[0m\u001b[32m:\u001b[0m\u001b[32m \"\u001b[0m\u001b[32mHistory\u001b[0m\u001b[32m of\u001b[0m\u001b[32m Micro\u001b[0m\u001b[32mstrategy\u001b[0m\u001b[32m company\u001b[0m\u001b[32m\"\n",
      "\u001b[0mObservation: \u001b[33mMicroStrategy was founded in 1989 and has a notable legacy of innovation in web and business intelligence. In recent years, the company has introduced thousands of transformative features and enhancements to empower customers to make smarter and faster decisions. MicroStrategy 2019 is the company's latest platform release, featuring HyperIntelligence, transformational mobility, and federated analytics.\u001b[0m\n",
      "Thought: \u001b[32mI\u001b[0m\u001b[32m still\u001b[0m\u001b[32m need\u001b[0m\u001b[32m to\u001b[0m\u001b[32m find\u001b[0m\u001b[32m the\u001b[0m\u001b[32m answer\u001b[0m\u001b[32m to\u001b[0m\u001b[32m the\u001b[0m\u001b[32m question\u001b[0m\u001b[32m about\u001b[0m\u001b[32m the\u001b[0m\u001b[32m founder\u001b[0m\u001b[32m of\u001b[0m\u001b[32m Micro\u001b[0m\u001b[32mstrategy\u001b[0m\u001b[32m.\n",
      "\u001b[0m\u001b[32mTool\u001b[0m\u001b[32m:\u001b[0m\u001b[32m Search\u001b[0m\u001b[32m_in\u001b[0m\u001b[32m_documents\u001b[0m\u001b[32m\n",
      "\u001b[0m\u001b[32mTool\u001b[0m\u001b[32m Input\u001b[0m\u001b[32m:\u001b[0m\u001b[32m \"\u001b[0m\u001b[32mMicro\u001b[0m\u001b[32mstrategy\u001b[0m\u001b[32m leadership\u001b[0m\u001b[32m team\u001b[0m\u001b[32m\"\n",
      "\u001b[0mObservation: \u001b[33mThe MicroStrategy leadership team includes Michael J. Saylor, who serves as Chairman, President, and Chief Executive Officer. This information can be found in Document[3].\u001b[0m\n",
      "Thought: \u001b[32mI\u001b[0m\u001b[32m have\u001b[0m\u001b[32m found\u001b[0m\u001b[32m the\u001b[0m\u001b[32m answer\u001b[0m\u001b[32m to\u001b[0m\u001b[32m the\u001b[0m\u001b[32m question\u001b[0m\u001b[32m.\u001b[0m\u001b[32m \n",
      "\u001b[0m\u001b[32mFinal\u001b[0m\u001b[32m Answer\u001b[0m\u001b[32m:\u001b[0m\u001b[32m Michael\u001b[0m\u001b[32m J\u001b[0m\u001b[32m.\u001b[0m\u001b[32m S\u001b[0m\u001b[32maylor\u001b[0m\u001b[32m is\u001b[0m\u001b[32m the\u001b[0m\u001b[32m founder\u001b[0m\u001b[32m of\u001b[0m\u001b[32m Micro\u001b[0m\u001b[32mstrategy\u001b[0m\u001b[32m.\u001b[0m\n",
      " Who founded Microstrategy companie?  \n",
      " Michael J. Saylor is the founder of Microstrategy.\n"
     ]
    }
   ],
   "source": [
    "def run_chatbox():\n",
    "    #query = 'first query'\n",
    "    chat_history=[]\n",
    "    while True:\n",
    "        try:\n",
    "            query = input('User: ')\n",
    "            if query == 'exit': \n",
    "                return chat_history\n",
    "            chat_history = use_list_dict(chat_history, query, isquery=True)\n",
    "            result = agent.run(query =use_list_dict(chat_history)) #json.dumps(chat_history)\n",
    "            answer = result[\"answers\"][0].answer\n",
    "            chat_history = use_list_dict(chat_history, answer, isquery=False)\n",
    "            print('\\n', query, ' \\n', result[\"answers\"][0].answer)\n",
    "        except:\n",
    "            print('error')\n",
    "\n",
    "chat_history = run_chatbox()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'user', 'conten': 'Who are the founders of Paratek Pharmaceuticals?'}]\n"
     ]
    }
   ],
   "source": [
    "print(chat_history)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
