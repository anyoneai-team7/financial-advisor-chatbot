{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.document_stores import ElasticsearchDocumentStore\n",
    "import os\n",
    "from haystack import Pipeline\n",
    "from haystack.nodes import PreProcessor\n",
    "from src import download_data, config, preprocess_data_raw, preprocess_data_raw_2\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data for AWS bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = config.BUCKET_NAME\n",
    "s3_folder = config.S3_FOLDER_URL\n",
    "local_dir = config.DATA_RAW_PATH\n",
    "\n",
    "# load data form AWS bucket and save in local_dir\n",
    "download_data.download_s3_folder(bucket_name, s3_folder, \n",
    "                                 local_dir=local_dir, \n",
    "                                 sample_size=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save data set as jason in .data/processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_data_raw_2.convert_pdf_jason(config.DATA_RAW_PATH, config.DATA_PROCESSED_PATH)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generated the Elastisearch document store\n",
    "Before we need to run the elastisearch container using just once:\n",
    "\n",
    "**docker pull docker.elastic.co/elasticsearch/elasticsearch:7.9.2**\n",
    "\n",
    "And then run image:\n",
    "\n",
    "**docker run -d -p 9200:9200 -e \"discovery.type=single-node\" elasticsearch:7.9.2**\n",
    "\n",
    "You can do this manually, or using our [launch_es()](https://docs.haystack.deepset.ai/reference/utils-api) utility function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the host where Elasticsearch is running, default to localhost\n",
    "host = os.environ.get(\"ELASTICSEARCH_HOST\", \"localhost\")\n",
    "\n",
    "document_store = ElasticsearchDocumentStore(\n",
    "    host=host,\n",
    "    username=\"\",\n",
    "    password=\"\",\n",
    "    index=\"document\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset from json_dataset.json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file_path = os.path.join(config.DATA_PROCESSED_PATH, \"json_dataset.json\")\n",
    "# Load the documents from the JSON file\n",
    "with open(json_file_path, \"r\") as f:\n",
    "    document_list = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_list[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing and loading into the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d023bfe4ab1247be97d141deb610caf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preprocessing:   0%|          | 0/10 [00:00<?, ?docs/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We found one or more sentences whose word count is higher than the split length.\n",
      "Document 58149ab146d0b3889605e662512296b9 is 27615 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time.\n",
      "Document cf8c146f296576b1988b1cefaac9f500 is 320800 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time.\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of PreProcessor\n",
    "# Each document is divided into paragraphs of approximately 500 tokens.\n",
    "\n",
    "preprocessor = PreProcessor(\n",
    "    clean_empty_lines=True,\n",
    "    clean_whitespace=True,\n",
    "    clean_header_footer=True,\n",
    "    split_by=\"word\",\n",
    "    split_length=500,\n",
    "    split_overlap=20,\n",
    "    split_respect_sentence_boundary=True,\n",
    ")\n",
    "\n",
    "doc_processed = preprocessor.process(document_list)\n",
    "\n",
    "# clear the document store before loading documents\n",
    "document_store.delete_documents()\n",
    "\n",
    "# Upload documents to the document store\n",
    "document_store.write_documents(doc_processed)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_processed[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate retriver and PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.nodes import BM25Retriever\n",
    "# Create an instance of BM25Retriver to select top_k better documents into document_store\n",
    "retriever = BM25Retriever(document_store=document_store, top_k = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.nodes import PromptTemplate\n",
    "\n",
    "# Create an instance of PromTemplate, we use the type question-answering\n",
    "qa_prompt = PromptTemplate(\n",
    "            name=\"question-answering\",\n",
    "            prompt_text=\"\"\"Given the context please answer the question. \n",
    "            Your answer should be in your own words and be no longer than 50 words. \n",
    "            Context: {join(documents)}; \n",
    "            Question:  {query}; Answer:\"\"\",\n",
    "                    )\n",
    "\n",
    "#prompt_node = PromptNode(model_name_or_path=\"google/flan-t5-large\", default_prompt_template=lfqa_prompt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.nodes import PromptNode\n",
    "\n",
    "# Create an instance of PromptNode using ChatGPT API as generator\n",
    "generator = PromptNode(\"gpt-3.5-turbo\", \n",
    "                         api_key = config.API_KEY,\n",
    "                         default_prompt_template = qa_prompt,\n",
    "                         model_kwargs={\"stream\":True}) \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline Retriver-Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline()\n",
    "pipe.add_node(component=retriever, name=\"retriever\", inputs=[\"Query\"])\n",
    "pipe.add_node(component=generator, name=\"generator\", inputs=[\"retriever\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AMERI Holdings provides SAP cloud, digital and enterprise services to clients worldwide, with a primary business objective of enhancing their clients' business capabilities and technologies through consulting services and strategic acquisitions.[\"AMERI Holdings provides SAP cloud, digital and enterprise services to clients worldwide, with a primary business objective of enhancing their clients' business capabilities and technologies through consulting services and strategic acquisitions.\"]\n"
     ]
    }
   ],
   "source": [
    "# Queries examples: use some companie name\n",
    "output = pipe.run(query=\"What is the company AMERI Holdings dedicated to?\", \n",
    "                      params={\"retriever\": {\"top_k\": 2},}) # Set params value is optative, because top_k was fixed as 5 before\n",
    "print(output['results'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
